<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="ReMedQA: Are We Done with Medical Multiple-Choice Benchmarks?">
  <meta name="keywords" content="mathvista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ReMedQA: Are We Done With Medical Multiple-Choice Benchmarks?</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<style>
  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
    padding: 20px;
    background: #f6f8fa;
  }

  .table-container {
    overflow-x: auto;
    background: white;
    border-radius: 6px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.12);
    padding: 20px;
  }

  table {
    border-collapse: collapse;
    width: 100%;
    font-size: 13px;
  }

  th,
  td {
    padding: 8px 6px;
    text-align: right;
    border-right: 1px solid #d0d7de;
  }

  th:first-child,
  td:first-child {
    text-align: left;
    font-weight: 500;
    border-right: 2px solid #24292f;
  }

  th {
    background: #f6f8fa;
    font-weight: 600;
    border-bottom: 2px solid #d0d7de;
  }

  .header-row th {
    font-size: 11px;
    padding: 4px 6px;
  }

  .metric-row th {
    font-family: 'Courier New', monospace;
    font-size: 11px;
    font-weight: 500;
  }

  .section-header {
    background: #f6f8fa;
    font-weight: 700;
    text-align: center !important;
    border-top: 2px solid #24292f;
    border-bottom: 1px solid #d0d7de;
  }

  .avg-col {
    background: #eff6ff !important;
    font-weight: 600;
  }

  .best {
    font-weight: 900;
  }

  .second-best {
    text-decoration: underline;
  }

  .dashed-row td {
    border-top: 1px dashed #d0d7de;
  }

  .icon {
    color: #6e7781;
    font-size: 11px;
    margin-left: 4px;
  }

  .medical {
    color: #0969da;
  }

  tr:hover {
    background: #f6f8fa;
  }

  tbody tr:last-child td {
    border-bottom: 2px solid #24292f;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
              <span class="mathvista" style="vertical-align: middle">ReMedQA</span>
            </h1>
            <h2 class="subtitle is-3 publication-subtitle">
              Are We Done With Medical Multiple-Choice Benchmarks?
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://disi-unibo-nlp.github.io/people/">Alessio Cocchieri</a>,
              </span>
              <span class="author-block">
                <a href="https://disi-unibo-nlp.github.io/people/">Luca Ragazzi</a>,
              </span>
              <span class="author-block">
                <a href="https://disi-unibo-nlp.github.io/people/">Giuseppe Tagliavini</a>,
              </span>
              <span class="author-block">
                <a href="https://disi-unibo-nlp.github.io/people/">Gianluca Moro</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">University of Bologna, Italy</span><br>
              <span class="paper-block"><b style="color:#f41c1c">EACL 2026 Main Track</b></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://github.com/disi-unibo-nlp/remedqa"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://huggingface.co/datasets/disi-unibo-nlp/ReMedQA/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="https://disi-unibo-nlp.github.io/remedqa/#leaderboard"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <p style="font-size:18px">üèÜ</p>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
                <!-- Site Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://disi-unibo-nlp.github.io"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <!-- üíªüîó -->
                      <p style="font-size:18px">üåê</p>
                    </span>
                    <span>Research Team</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="images/abstract.png" alt="overall" width="40%" />

            </div>
          </div>



          <!-- <div class="box m-5">
            <div class="content has-text-centered">
              <img src="images/perturbations.png" alt="overall3" width="40%"/>
              <p> <b>Semantic-preserving MCQA perturbations in ReMedQA.</b> Consistent predictions across sets indicate model reliability. the correct answer is highlighted.</p>
            </p>
            </div>
          </div> -->

        </div>
      </div>
  </section>


  <section class="section">
    <div class="container" style="margin-bottom: 2vh;">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">

            <p>
              Medical multiple-choice question answering (MCQA) benchmarks report near-human accuracy, with some
              approaching saturation and fueling claims of clinical readiness. Yet <strong>a single accuracy score is a
                poor
                proxy for competence</strong>: models that change answers under minor perturbations cannot be considered
              reliable.
              We argue that <strong>reliability underpins accuracy</strong>‚Äîonly consistent predictions make correctness
              meaningful.
            </p>
            <p>
              We release <strong>ReMedQA</strong>, a benchmark suite that augments three standard medical MCQA datasets
              with open-answer
              variants and systematically perturbed items. Building on this, we introduce <strong>ReAcc</strong> and
              <strong>ReCon</strong>, two
              reliability metrics: ReAcc measures the proportion of questions answered accurately across all variations,
              while ReCon measures the proportion answered consistently regardless of correctness.
            </p>
            <p>
              Our evaluation shows that <strong>high MCQA accuracy masks low reliability</strong>: models remain
              sensitive to format and
              perturbation changes, and domain specialization offers no robustness gain. MCQA underestimates smaller
              models while inflating large ones that exploit structural cues‚Äîwith some <strong>producing correct answers
                without
                seeing the question</strong>.
            </p>
            <p>
              These findings show that, despite near-saturated accuracy, <strong>we are not yet done with medical
                multiple-choice benchmarks</strong>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- LEADERBOARD SECTION -->
  <section class="hero is-light is-small" id="leaderboard">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">
        <span class="mathvista" style="vertical-align: middle">Leaderboard on ReMedQA</span>
      </h1>
    </div>
  </section>

  <section class="section">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-full has-text-centered content">

          <!--      <h2 class="title is-3">Textual Problems</h2>
<div class="content">
  <b>CE, C1, C2, L1, L2, GP, HC:</b> Accuracy across age groups.  
  <br>
  <b>Avg:</b> Overall average.  
  <br> -->



          <table>
            <thead>
              <tr class="header-row">
                <th rowspan="2"><strong>ReMedQA</strong></th>
                <th colspan="2"><em>Pro Med.</em></th>
                <th colspan="2"><em>College Med.</em></th>
                <th colspan="2"><em>Anatomy</em></th>
                <th colspan="2"><em>Clinical</em></th>
                <th colspan="2"><em>Biology</em></th>
                <th colspan="2"><em>Genetics</em></th>
                <th colspan="2"><strong>MMLU Avg</strong></th>
                <th colspan="2"><strong>MedQA</strong></th>
                <th colspan="2"><strong>MedMCQA</strong></th>
                <th colspan="2" class="avg-col"><strong>Avg</strong></th>
              </tr>
              <tr class="metric-row">
                <th>ReAcc</th>
                <th>ReCon</th>
                <th>ReAcc</th>
                <th>ReCon</th>
                <th>ReAcc</th>
                <th>ReCon</th>
                <th>ReAcc</th>
                <th>ReCon</th>
                <th>ReAcc</th>
                <th>ReCon</th>
                <th>ReAcc</th>
                <th>ReCon</th>
                <th><strong>ReAcc</strong></th>
                <th><strong>ReCon</strong></th>
                <th><strong>ReAcc</strong></th>
                <th><strong>ReCon</strong></th>
                <th><strong>ReAcc</strong></th>
                <th><strong>ReCon</strong></th>
                <th class="avg-col"><strong>ReAcc</strong></th>
                <th class="avg-col"><strong>ReCon</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td colspan="21" class="section-header"><strong>Large Models</strong></td>
              </tr>
              <tr>
                <td>GPT-5-mini <span class="icon">üß†</span></td>
                <td>76.4</td>
                <td>77.5</td>
                <td>58.3</td>
                <td>66.7</td>
                <td>65.6</td>
                <td>73.6</td>
                <td>61.1</td>
                <td>68.9</td>
                <td>67.0</td>
                <td>71.3</td>
                <td>85.4</td>
                <td>86.6</td>
                <td>69.0</td>
                <td>74.1</td>
                <td>65.9</td>
                <td>68.2</td>
                <td>39.1</td>
                <td>46.0</td>
                <td class="avg-col best">58.0</td>
                <td class="avg-col best">62.8</td>
              </tr>
              <tr>
                <td>GPT-OSS-120B <span class="icon">üß†</span></td>
                <td>72.0</td>
                <td>76.8</td>
                <td>59.8</td>
                <td>65.9</td>
                <td>54.4</td>
                <td>65.6</td>
                <td>57.0</td>
                <td>65.3</td>
                <td>70.6</td>
                <td>74.3</td>
                <td>79.3</td>
                <td>84.1</td>
                <td>65.5</td>
                <td>72.0</td>
                <td>62.4</td>
                <td>67.6</td>
                <td>34.4</td>
                <td>43.9</td>
                <td class="avg-col second-best">54.1</td>
                <td class="avg-col second-best">61.2</td>
              </tr>
              <tr>
                <td>Gemini-2.5-Flash <span class="icon">üß†</span></td>
                <td>57.9</td>
                <td>60.5</td>
                <td>46.2</td>
                <td>55.7</td>
                <td>52.0</td>
                <td>62.4</td>
                <td>46.6</td>
                <td>52.8</td>
                <td>60.6</td>
                <td>67.9</td>
                <td>72.0</td>
                <td>73.2</td>
                <td>55.9</td>
                <td>62.1</td>
                <td>47.6</td>
                <td>50.7</td>
                <td>33.2</td>
                <td>39.4</td>
                <td class="avg-col">45.6</td>
                <td class="avg-col">50.7</td>
              </tr>
              <tr>
                <td>GPT-OSS-20B <span class="icon">üß†</span></td>
                <td>62.6</td>
                <td>66.7</td>
                <td>47.7</td>
                <td>55.7</td>
                <td>52.0</td>
                <td>59.2</td>
                <td>48.7</td>
                <td>54.9</td>
                <td>60.6</td>
                <td>65.1</td>
                <td>64.6</td>
                <td>75.3</td>
                <td>56.0</td>
                <td>62.8</td>
                <td>49.4</td>
                <td>54.0</td>
                <td>24.9</td>
                <td>30.4</td>
                <td class="avg-col">43.4</td>
                <td class="avg-col">49.1</td>
              </tr>
              <tr>
                <td>Llama-3.3-70B</td>
                <td>53.5</td>
                <td>56.5</td>
                <td>34.8</td>
                <td>40.9</td>
                <td>48.8</td>
                <td>53.6</td>
                <td>42.5</td>
                <td>45.6</td>
                <td>55.0</td>
                <td>56.0</td>
                <td>63.4</td>
                <td>64.6</td>
                <td>49.7</td>
                <td>52.9</td>
                <td>34.0</td>
                <td>37.9</td>
                <td>23.6</td>
                <td>28.6</td>
                <td class="avg-col">35.8</td>
                <td class="avg-col">39.8</td>
              </tr>
              <tr>
                <td colspan="21" class="section-header"><strong>Small Models</strong></td>
              </tr>
              <tr>
                <td>Llama-3-8B</td>
                <td>9.8</td>
                <td>17.7</td>
                <td>9.8</td>
                <td>15.9</td>
                <td>12.8</td>
                <td>27.2</td>
                <td>11.9</td>
                <td>20.2</td>
                <td>15.6</td>
                <td>25.7</td>
                <td>23.2</td>
                <td>37.8</td>
                <td>13.9</td>
                <td>24.1</td>
                <td>6.1</td>
                <td>11.9</td>
                <td>6.8</td>
                <td>16.0</td>
                <td class="avg-col">8.9</td>
                <td class="avg-col">17.3</td>
              </tr>
              <tr>
                <td>Llama3-Med42-8B <span class="icon medical">üíö</span></td>
                <td>15.7</td>
                <td>18.9</td>
                <td>18.2</td>
                <td>24.6</td>
                <td>20.8</td>
                <td>29.6</td>
                <td>17.6</td>
                <td>24.4</td>
                <td>15.6</td>
                <td>22.9</td>
                <td>24.4</td>
                <td>36.6</td>
                <td>18.7</td>
                <td>26.2</td>
                <td>9.1</td>
                <td>13.2</td>
                <td>10.8</td>
                <td>18.2</td>
                <td class="avg-col second-best">12.9</td>
                <td class="avg-col second-best">19.2</td>
              </tr>
              <tr class="dashed-row">
                <td>Phi-3.5-mini</td>
                <td>23.6</td>
                <td>25.6</td>
                <td>9.8</td>
                <td>24.2</td>
                <td>26.4</td>
                <td>35.2</td>
                <td>18.1</td>
                <td>26.9</td>
                <td>29.4</td>
                <td>32.1</td>
                <td>19.5</td>
                <td>40.2</td>
                <td>21.1</td>
                <td>30.7</td>
                <td>12.3</td>
                <td>15.4</td>
                <td>11.0</td>
                <td>13.8</td>
                <td class="avg-col best">14.8</td>
                <td class="avg-col best">20.0</td>
              </tr>
              <tr>
                <td>MediPhi-3.8B <span class="icon medical">üíö</span></td>
                <td>11.8</td>
                <td>16.1</td>
                <td>14.4</td>
                <td>18.2</td>
                <td>20.0</td>
                <td>27.2</td>
                <td>18.7</td>
                <td>20.2</td>
                <td>21.1</td>
                <td>23.9</td>
                <td>24.4</td>
                <td>29.3</td>
                <td>18.4</td>
                <td>22.5</td>
                <td>8.6</td>
                <td>13.4</td>
                <td>7.9</td>
                <td>11.5</td>
                <td class="avg-col">11.6</td>
                <td class="avg-col">15.8</td>
              </tr>
              <tr class="dashed-row">
                <td>Gemma-3-4B</td>
                <td>5.9</td>
                <td>8.3</td>
                <td>5.3</td>
                <td>9.8</td>
                <td>16.0</td>
                <td>19.2</td>
                <td>6.7</td>
                <td>9.4</td>
                <td>12.8</td>
                <td>17.4</td>
                <td>23.2</td>
                <td>23.8</td>
                <td>11.7</td>
                <td>14.6</td>
                <td>3.3</td>
                <td>6.2</td>
                <td>3.8</td>
                <td>6.8</td>
                <td class="avg-col">6.3</td>
                <td class="avg-col">9.2</td>
              </tr>
              <tr>
                <td>MedGemma-4B <span class="icon medical">üíö</span></td>
                <td>2.8</td>
                <td>4.3</td>
                <td>2.3</td>
                <td>5.3</td>
                <td>4.8</td>
                <td>10.4</td>
                <td>3.6</td>
                <td>5.7</td>
                <td>6.4</td>
                <td>9.2</td>
                <td>8.5</td>
                <td>11.0</td>
                <td>4.7</td>
                <td>7.6</td>
                <td>2.5</td>
                <td>5.8</td>
                <td>2.7</td>
                <td>5.3</td>
                <td class="avg-col">3.3</td>
                <td class="avg-col">6.2</td>
              </tr>
            </tbody>
          </table>


          <p style="font-size: 14px;">
            <strong>Note:</strong> üß† Reasoning-focused LLMs; üíö Medical-specialized LLMs.
          </p>
        </div>

      </div>
    </div>

    </div>
  </section>



  <!-- DATASET SECTION -->
  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">
        <span class="mathvista" style="vertical-align: middle">Methodology</span>
      </h1>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered is-vcentered">

        <!-- FIGURA A SINISTRA -->
        <div class="column is-3 has-text-centered">
          <figure class="image">
            <img src="images/perturbations.png" alt="Methodology overview">
          </figure>
        </div>

        <!-- TESTO A DESTRA -->
        <div class="column is-7">
          <h2 class="title is-3">How We Designed ReMedQA</h2>

          <div class="content has-text-justified">

            <p>
              ReMedQA evaluates medical AI models in two key ways: <strong>Closed format</strong>,
              where models choose from multiple-choice options, and <strong>Open format</strong>,
              where models generate free-form answers that are then matched to the original options.
              This dual approach helps us understand how reliably models can answer medical questions.
            </p>

            <p>
              To truly test reliability, we created five different versions of each question that mean
              the same thing but look different on the surface. If a model is truly reliable, it should
              give consistent answers across all these variations. Think of it like asking the same
              medical question in slightly different ways - a trustworthy expert should recognize they're
              the same question and give the same answer every time.
            </p>
            We considered the following closed (<strong>MCQA</strong>) perturbations:
            <ul>

              <li>
                <strong>No Labels.</strong>
                Standard answer labels (A/B/C/D) are removed, forcing models to output
                the content of the correct option rather than a symbol.
              </li>

              <li>
                <strong>Roman Numerals.</strong>
                Standard labels are replaced with
                <strong>I, II, III, IV</strong>, testing invariance to alternative
                labeling schemes.
              </li>

              <li>
                <strong>Fixed Position.</strong>
                We always place the gold option in position <strong>D</strong>.
                We select D because it is the least frequent gold position in the datasets, minimizing positional priors
                while stressing
                robustness to order.
              </li>

              <li>
                <strong>Select Incorrect.</strong>
                Models must choose all incorrect options instead of the gold one,
                probing whether they can robustly identify distractors.
              </li>

              <li>
                <strong>None Provided.</strong>
                The gold option is replaced with the string
                <em>"None of the provided options"</em>, requiring models to recognize
                when the correct answer is absent from the listed options.
              </li>
            </ul>
          </div>


        </div>
      </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="content has-text-justified">
        <h2 class="title is-3">Datasets</h2>

        <p>
          To build ReMedQA, we draw from three prominent medical corpora, yielding eight English-language MCQA tasks
          spanning specialties
          such as genetics, anatomy, and clinical reasoning.
          These datasets reflect both real-world scenarios encountered by medical professionals and exam formats
          commonly used in licensing and entrance tests.
          We focus only on datasets with 4-option choice format, to ensure consistency across our findings and
          structural compatibility with our framework.
        </p>

        <ul>
          <li>
            <strong>MedQA</strong>:
            A dataset of US Medical Licensing Exam (USMLE) style questions covering basic sciences, clinical knowledge,
            and medical reasoning.
          </li>
          <li>
            <strong>MedMCQA</strong>:
            A large-scale dataset of medical entrance exam questions from India, spanning various medical subjects and
            designed to test comprehensive medical knowledge.
          </li>
          <li>
            <strong>MMLU Medical Subset</strong>:
            A subset of the Massive Multitask Language Understanding benchmark focusing on six medical topics, including
            clinical knowledge, medical genetics, anatomy, professional medicine, college biology, and college medicine
          </li>
      </div>
    </div>

    <div class="box m-5">
      <div class="content has-text-centered">

        <div style="width: 80%; margin: 0 auto;">
          <img src="images/datasets.png" alt="overall" style="width: 100%;" />


        </div>

      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="content has-text-justified">
        <h2 class="title is-3">Metrics</h2>

        <p>
          To measure how reliable medical AI models really are, we created two key metrics that go
          beyond simple accuracy. These metrics check whether models give consistent and correct
          answers across all seven versions of each question.
        </p>

        <ul>
          <li>
            <strong>ReCon (Reliability through Consistency):</strong>
            Measures if a model gives the <em>same answer</em> to all versions of a question.
            Think of it like this: if you ask a doctor the same medical question in seven different
            ways, they should recognize it's the same question and give the same answer every time.
            A model scores points only when it's perfectly consistent across all formats - no
            flip-flopping allowed!
          </li>

          <li>
            <strong>ReAcc (Reliability through Correctness):</strong>
            This is even stricter - it measures if a model gets the <em>correct answer</em> for
            all versions of a question. Consistency isn't enough if you're consistently wrong!
            This metric only gives credit when the model is both reliable AND accurate across
            every single variation. It's the gold standard for trustworthy medical AI.
          </li>
        </ul>

        <p>
          Together, these metrics reveal the true reliability of medical AI models. A high standard
          accuracy score might look impressive, but ReCon and ReAcc show whether that performance is
          stable and dependable - exactly what we need for real-world healthcare applications.
        </p>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">
        <span class="mathvista" style="vertical-align: middle">Main Findings</span>
      </h1>
    </div>
  </section>

  <section class="section">
    <div class="container" style="margin-top: -50px; margin-bottom: -100px;">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="images/options_only.png" alt="overall" width="30%" />

              </div>
            </div>

            <div class="box m-5">
              <div class="content has-text-centered">

                <div class="content has-text-centered">
                  <img src="images/consistency_perturbations.png" alt="overall" width="100%" />

                </div>

              </div>
            </div>

            <!-- <div class="box m-5">
            <div class="content has-text-centered">
              <img src="images/perturbations.png" alt="overall3" width="40%"/>
              <p> <b>Semantic-preserving MCQA perturbations in ReMedQA.</b> Consistent predictions across sets indicate model reliability. the correct answer is highlighted.</p>
            </p>
            </div>
          </div> -->
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="content has-text-justified">

        <h3 class="title is-4">Does High Accuracy Mean High Reliability?</h3>
        <ul>
          <li>
            <p>
              Our results reveal a surprising disconnect: <strong>models can score very high on accuracy
                but still fail dramatically on reliability</strong>. Even when models answer most questions
              correctly, they often give different answers when we perturbate the same question. This instability is
              especially pronounced in smaller models, whose
              predictions can flip completely based on minor format changes.
            </p>
          </li>
          <li>
            <p>
              We also found a <strong>systematic gap between open-ended and multiple-choice performance</strong>.
              Large models benefit from the structured format of multiple-choice questions, while smaller
              models actually perform worse with multiple choices than when generating free-form answers -
              suggesting they get confused by the extra options and formatting.
            </p>
          </li>
          <li>
            <p>
              Most importantly, even frontier models with near-perfect accuracy scores show only
              <strong>modest reliability</strong>. The gap between their accuracy and reliability metrics
              reveals that <em>consistency - not accuracy - is the real bottleneck</em> for building
              trustworthy medical AI. A model might know the right answer, but if it can't reliably give
              that answer across different presentations of the same question, it's not ready for real-world
              healthcare use.
            </p>
          </li>
        </ul>


        <h3 class="title is-4">Can Models Answer Without Reading the Question?</h3>
        <ul>
          <li>
            <p>
              We tested something surprising: what happens if we give models only the answer choices
              without showing them the actual medical question? Shockingly, models still scored 40-50%
              correct - far better than random guessing! This means they're picking up on patterns and
              shortcuts in how answers are written, rather than truly understanding the medicine.
            </p>
          </li>
          <li>
            <p>
              However, when we apply our reliability tests to these "options-only" answers, the models'
              consistency completely falls apart. This proves they're using superficial tricks rather than
              real medical knowledge - a serious concern for healthcare applications.
            </p>
          </li>
        </ul>

        <h3 class="title is-4">Which Question Variations Are Hardest?</h3>
        <ul>
          <li>
            <p>
              Our analysis reveals that the <strong>"None Provided"</strong> variation - where the correct
              answer is replaced with "None of the provided options" - causes the biggest drop in consistency
              across all models. This forces models to reason about what's missing rather than just picking
              from what's there.
            </p>
          </li>
          <li>
            <p>
              Interestingly, the <strong>"Select Incorrect"</strong> variation (asking for wrong answers
              instead of right ones) mainly trips up smaller models. Larger models can handle this task
              reversal much better, showing they understand the task more deeply.
            </p>
          </li>
          <li>
            <p>
              Across different medical datasets like MedQA and MedMCQA, models show similar consistency
              patterns within each format. However, MedMCQA becomes much harder when models need to stay
              consistent across multiple formats at once - revealing hidden complexity that single-format
              testing misses.
            </p>
          </li>
          <li>
            <p>
              The bottom line: <strong>OpenAI's models</strong> (GPT-5-mini, GPT-OSS-120B) remain the most
              reliable across all our tests, while smaller models show bigger drops in consistency, confirming
              that reliability still improves with model size.
            </p>
          </li>
        </ul>
      </div>
    </div>
  </section>



  <!--bibtex -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3 has-text-centered">BibTeX</h2>
      <pre><code>@inproceedings{cocchieri-etal-2026-remedqa,
    title = "ReMedQA: Are We Done With Medical Multiple-Choice Benchmarks?",
    author = "Cocchieri, Alessio  and
      Ragazzi, Luca  and
      Tagliavini, Giuseppe  and
      Moro, Gianluca",
    booktitle = "Proceedings of the 19th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2026",
    address = "Rabat, Morocco",
    publisher = "Association for Computational Linguistics"
}</div>
</section>

<section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://disi.unibo.it/en" target="_blank" rel="external">
        <img class="center-block org-banner" src="images/unibo-logo-big.png">
    </a>
  </div>
</section>

<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            this website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>


<script>
  document.addEventListener('DOMContentLoaded', () => {
    bulmaCarousel.attach('.carousel', {
      slidesToScroll: 1,
      slidesToShow: 1,
      autoplay: false,
      loop: false
    });
  });
</script>
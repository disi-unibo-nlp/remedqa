<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ReMedQA: Are We Done With Medical Multiple-Choice Benchmarks?">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ReMedQA: Are We Done With Medical Multiple-Choice Benchmarks?</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <span class="mathvista" style="vertical-align: middle">ReMedQA</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Are We Done With Medical Multiple-Choice Benchmarks?
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://disi-unibo-nlp.github.io/people/">Alessio Cocchieri</a>,
            </span>
            <span class="author-block">
              <a href="https://disi-unibo-nlp.github.io/people/">Luca Ragazzi</a>,
            </span>
            <span class="author-block">
              <a href="https://disi-unibo-nlp.github.io/people/">Giuseppe Tagliavini</a>,
            </span>
            <span class="author-block">
              <a href="https://disi-unibo-nlp.github.io/people/">Gianluca Moro</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Bologna, Italy</span><br>
            <span class="paper-block"><b style="color:#f41c1c">EACL 2026 Main Track</b></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/disi-unibo-nlp/remedqa"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a target="_blank" href="https://huggingface.co/datasets/disi-unibo-nlp/ReMedQA/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="https://disi-unibo-nlp.github.io/remedqa/#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üèÜ</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
              <!-- Site Link. -->
              <span class="link-block">
                <a target="_blank" href="https://disi-unibo-nlp.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <!-- üíªüîó -->
                      <p style="font-size:18px">üåê</p>
                  </span>
                  <span>Research Team</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="images/scores.png" alt="overall" width="50%" />
              <p>
                <b>LLM evaluation on ReMedQA.</b> Colored: MCQA accuracy; white: open-answer accuracy. ReAcc (green): % of questions answered accurately across all variations; ReCon (black): % of questions answered consistently (same prediction) across all variations.
              </p>
            </div>
          </div>
          
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="images/scores2.png" alt="overall2" width="50%"/>
              <p> <b>Options Only performance.</b> Each model is evaluated by providing only the MCQA answer options, without the question prompt. The red dashed line indicates the random-guess baseline.
              </p>
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="images/perturbations.png" alt="overall3" width="40%"/>
              <p> <b>Semantic-preserving MCQA perturbations in ReMedQA.</b> Consistent predictions across sets indicate model reliability. The correct answer is highlighted.</p>
            </p>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          
        <p>
            Medical multiple-choice question answering (MCQA) benchmarks report near-human accuracy, with some approaching saturation and fueling claims of clinical readiness. Yet a single accuracy score is a poor proxy for competence: models that change answers under minor perturbations cannot be considered reliable. We argue that reliability underpins accuracy‚Äîonly consistent predictions make correctness meaningful.
            </p>
          <p> 
            We release ReMedQA, a benchmark suite that augments three standard medical MCQA datasets with open-answer variants and systematically perturbed items. Building on this, we introduce ReAcc and ReCon, two reliability metrics: ReAcc measures the proportion of questions answered accurately across all variations, while ReCon measures the proportion answered consistently regardless of correctness.
            </p>
          <p> 
            Our evaluation shows that high MCQA accuracy masks low reliability: models remain sensitive to format and perturbation changes, and domain specialization offers no robustness gain. MCQA underestimates smaller models while inflating large ones that exploit structural cues‚Äîwith some producing correct answers without seeing the question.
            </p>
           <p>
            These findings show that, despite near-saturated accuracy, we are not yet done with medical multiple-choice benchmarks. 
            </p> 
        </div>
      </div>
    </div>
</div>
</section>


<!-- DATASET SECTION -->
<!-- <section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <span class="mathvista" style="vertical-align: middle">ReAcc and ReCon</span>
  </h1>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            <span class="mathvista">ReAcc</span> measures whether all predictions are correct across all versions:
            <span class="mathvista">ReCon</span> measures whether predictions remain invariant across all versions.
        </div>
      </div>
    </div>
</div>
  </div>
</section> -->


<!-- LEADERBOARD SECTION -->
<section class="hero is-light is-small" id="leaderboard">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <span class="mathvista" style="vertical-align: middle">Leaderboard on ReMedQA</span>
  </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">

        <h2 class="title is-3">Textual Problems</h2>
<div class="content">
  <b>CE, C1, C2, L1, L2, GP, HC:</b> Accuracy across age groups.  
  <br>
  <b>Avg:</b> Overall average.  
  <br>
  
  <table class="js-sort-table" id="results">
    <tr>
      <td><strong>Model</strong></td>
      <td><strong>CE</strong></td>
      <td><strong>C1</strong></td>
      <td><strong>C2</strong></td>
      <td><strong>L1</strong></td>
      <td><strong>L2</strong></td>
      <td><strong>GP</strong></td>
      <td><strong>HC</strong></td>
      <td><strong>Avg</strong></td>
    </tr>

    <tr><td colspan="9"><strong>Closed-Source</strong></td></tr>

    <tr><td>o3-mini-high üß†</td><td>83.8</td><td>82.0</td><td>81.7</td><td>80.6</td><td>79.2</td><td>77.1</td><td>73.3</td><td>79.7</td></tr>
    <tr><td>Gemini-2.0-Flash-T üß†</td><td>81.3</td><td>71.4</td><td>71.0</td><td>69.5</td><td>66.4</td><td>61.9</td><td>59.2</td><td>68.7</td></tr>
    <tr><td>Gemini-2.0-Flash</td><td>58.9</td><td>56.8</td><td>55.4</td><td>54.0</td><td>51.3</td><td>43.7</td><td>41.2</td><td>51.6</td></tr>
    <tr><td>Gemini-1.5-Pro</td><td>59.8</td><td>54.3</td><td>53.2</td><td>52.4</td><td>50.2</td><td>43.9</td><td>41.2</td><td>50.7</td></tr>
    <tr><td>Gemini-1.5-Flash</td><td>60.7</td><td>49.7</td><td>47.4</td><td>45.5</td><td>42.9</td><td>36.9</td><td>36.0</td><td>45.6</td></tr>
    <tr><td>GPT-4o</td><td>61.7</td><td>50.1</td><td>46.2</td><td>43.8</td><td>42.3</td><td>35.0</td><td>33.0</td><td>44.6</td></tr>
    <tr><td>GPT-4o-mini</td><td>49.5</td><td>42.2</td><td>42.8</td><td>41.5</td><td>39.8</td><td>31.4</td><td>30.0</td><td>40.4</td></tr>
    <tr><td>Gemini-1.5-Flash-8B</td><td>40.2</td><td>35.1</td><td>33.9</td><td>31.2</td><td>29.3</td><td>21.8</td><td>20.6</td><td>31.6</td></tr>

    <tr><td colspan="9"><strong>Open-Source &gt; 8B</strong></td></tr>

    <tr><td>DeepSeek-R1 üß†</td><td>85.0</td><td>77.3</td><td>75.9</td><td>74.7</td><td>72.7</td><td>69.7</td><td>69.0</td><td>74.9</td></tr>
    <tr><td>DeepSeek-V3</td><td>66.4</td><td>54.3</td><td>52.1</td><td>50.7</td><td>48.3</td><td>40.8</td><td>36.5</td><td>49.9</td></tr>
    <tr><td>Phi-4-14B *</td><td>66.4</td><td>51.9</td><td>48.1</td><td>46.0</td><td>43.7</td><td>37.0</td><td>32.2</td><td>46.5</td></tr>
    <tr><td>Phi-4-14B</td><td>59.8</td><td>50.4</td><td>45.9</td><td>43.6</td><td>41.1</td><td>33.6</td><td>30.0</td><td>43.5</td></tr>
    <tr><td>Qwen2.5-72B</td><td>53.3</td><td>48.4</td><td>45.2</td><td>43.3</td><td>41.4</td><td>34.4</td><td>29.6</td><td>42.2</td></tr>
    <tr><td>QwQ-32B üß†</td><td>56.1</td><td>43.5</td><td>40.0</td><td>37.3</td><td>34.4</td><td>25.4</td><td>23.2</td><td>37.1</td></tr>
    <tr><td>LLaMA-3.3-70B</td><td>44.9</td><td>41.5</td><td>39.7</td><td>37.3</td><td>35.7</td><td>26.3</td><td>26.2</td><td>35.9</td></tr>
    <tr><td>DeepSeek-R1-Qwen üß†</td><td>44.2</td><td>38.7</td><td>38.0</td><td>36.3</td><td>33.4</td><td>25.8</td><td>19.8</td><td>33.7</td></tr>

    <tr><td colspan="9"><strong>Open-Source ‚â§ 8B (Math-Specialized)</strong></td></tr>

    <tr><td>Qwen2.5-Math-7B * üîß</td><td>53.3</td><td>47.9</td><td>48.3</td><td>47.3</td><td>46.9</td><td>39.8</td><td>34.1</td><td>45.4</td></tr>
    <tr><td>Qwen2.5-Math-7B üîß</td><td>43.9</td><td>45.4</td><td>44.1</td><td>42.2</td><td>41.2</td><td>33.6</td><td>31.3</td><td>40.2</td></tr>
    <tr><td>NuminaMath-7B * üîß</td><td>43.0</td><td>38.8</td><td>38.0</td><td>36.7</td><td>35.3</td><td>26.7</td><td>24.5</td><td>34.7</td></tr>
    <tr><td>Qwen2.5-Math-7B *</td><td>40.7</td><td>37.0</td><td>38.3</td><td>36.7</td><td>35.6</td><td>27.5</td><td>26.1</td><td>34.6</td></tr>
    <tr><td>Qwen2.5-Math-7B</td><td>40.2</td><td>36.5</td><td>37.8</td><td>36.2</td><td>35.1</td><td>26.9</td><td>24.9</td><td>33.9</td></tr>
    <tr><td>NuminaMath-7B üîß</td><td>39.2</td><td>27.6</td><td>31.1</td><td>29.4</td><td>28.3</td><td>19.8</td><td>18.0</td><td>27.7</td></tr>
    <tr><td>NuminaMath-7B</td><td>28.8</td><td>25.6</td><td>24.9</td><td>24.1</td><td>23.1</td><td>25.4</td><td>22.4</td><td>24.9</td></tr>
    <tr><td>Mathstral-7B *</td><td>35.5</td><td>27.2</td><td>26.1</td><td>23.6</td><td>21.8</td><td>16.7</td><td>12.4</td><td>23.3</td></tr>
    <tr><td>NuminaMath-7B *</td><td>31.8</td><td>25.2</td><td>25.4</td><td>24.2</td><td>22.7</td><td>13.1</td><td>9.0</td><td>21.6</td></tr>
    <tr><td>DeepSeek-Math-7B * üîß</td><td>23.4</td><td>24.4</td><td>23.7</td><td>22.9</td><td>21.3</td><td>15.1</td><td>14.2</td><td>20.7</td></tr>
    <tr><td>Mathstral-7B</td><td>27.1</td><td>22.0</td><td>23.4</td><td>21.3</td><td>20.1</td><td>12.2</td><td>11.2</td><td>19.6</td></tr>
    <tr><td>DeepSeek-Math-7B *</td><td>21.3</td><td>21.6</td><td>21.9</td><td>20.7</td><td>19.6</td><td>13.2</td><td>10.2</td><td>18.4</td></tr>
    <tr><td>DeepSeek-Math-7B üîß</td><td>21.1</td><td>21.4</td><td>21.7</td><td>20.5</td><td>19.3</td><td>12.8</td><td>9.8</td><td>18.1</td></tr>
    <tr><td>DeepSeek-Math-7B</td><td>20.6</td><td>21.0</td><td>21.4</td><td>20.1</td><td>18.9</td><td>12.5</td><td>9.4</td><td>17.7</td></tr>
    <tr><td>ToRA-7B * üîß</td><td>12.2</td><td>11.6</td><td>12.1</td><td>11.5</td><td>11.1</td><td>7.6</td><td>6.4</td><td>10.4</td></tr>
    <tr><td>ToRA-7B üîß</td><td>6.5</td><td>11.1</td><td>12.4</td><td>11.8</td><td>11.3</td><td>9.3</td><td>7.7</td><td>10.0</td></tr>
  </table>

  <p style="font-size: 14px;">
    <strong>Note:</strong> üß† Reasoning-focused; * maj@8 instead of pass@1; üîß TIR mode.
  </p>
</div>

      </div>
    </div>

  </div>
</section>


<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">

        <h2 class="title is-3">Multimodal Problems</h2>
<div class="content">
  <b>CE, C1, C2, L1, L2, GP, HC:</b> Accuracy across age groups.  
  <br>
  <b>Avg:</b> Overall average.  
  <br>
  
  <table class="js-sort-table" id="results">
    <tr>
      <td><strong>Model</strong></td>
      <td><strong>CE</strong></td>
      <td><strong>C1</strong></td>
      <td><strong>C2</strong></td>
      <td><strong>L1</strong></td>
      <td><strong>L2</strong></td>
      <td><strong>GP</strong></td>
      <td><strong>HC</strong></td>
      <td><strong>Avg</strong></td>
    </tr>

    <tr><td colspan="9"><strong>Closed-Source</strong></td></tr>

    <tr>
      <td>Gemini-2.0-Flash-T üß†</td>
      <td>38.3</td><td>29.3</td><td>32.2</td><td>31.5</td><td>31.3</td><td>25.4</td><td>25.1</td><td>30.4</td>
    </tr>
    <tr>
      <td>Gemini-1.5-Pro</td>
      <td>30.4</td><td>25.5</td><td>24.2</td><td>21.3</td><td>20.4</td><td>18.4</td><td>15.3</td><td>22.2</td>
    </tr>
    <tr>
      <td>Gemini-1.5-Flash</td>
      <td>27.0</td><td>19.4</td><td>16.1</td><td>15.0</td><td>15.6</td><td>12.7</td><td>14.2</td><td>17.1</td>
    </tr>
    <tr>
      <td>GPT-4o</td>
      <td>25.2</td><td>20.4</td><td>17.8</td><td>14.8</td><td>12.9</td><td>10.2</td><td>10.9</td><td>16.0</td>
    </tr>
    <tr>
      <td>GPT-4o-mini</td>
      <td>23.5</td><td>18.5</td><td>16.1</td><td>13.4</td><td>12.1</td><td>10.2</td><td>11.5</td><td>15.0</td>
    </tr>
    <tr>
      <td>Gemini-1.5-Flash-8B</td>
      <td>18.3</td><td>14.3</td><td>12.3</td><td>11.3</td><td>11.3</td><td>9.2</td><td>10.9</td><td>12.5</td>
    </tr>

    <tr><td colspan="9"><strong>Open-Source &gt; 8B</strong></td></tr>

    <tr>
      <td>InternVL-2.5-38B-MPO</td>
      <td>19.1</td><td>21.0</td><td>19.7</td><td>17.1</td><td>16.4</td><td>12.7</td><td>12.0</td><td>16.9</td>
    </tr>
    <tr>
      <td>InternVL-2.5-38B</td>
      <td>14.8</td><td>14.7</td><td>13.6</td><td>11.5</td><td>9.9</td><td>7.4</td><td>6.6</td><td>11.2</td>
    </tr>
    <tr>
      <td>QVQ-72B üß†</td>
      <td>20.0</td><td>11.8</td><td>8.9</td><td>7.5</td><td>7.1</td><td>6.7</td><td>6.6</td><td>9.8</td>
    </tr>
    <tr>
      <td>Qwen2-VL-72B</td>
      <td>14.8</td><td>12.5</td><td>11.2</td><td>8.8</td><td>7.5</td><td>6.0</td><td>3.8</td><td>9.2</td>
    </tr>
    <tr>
      <td>Pixtral-12B *</td>
      <td>12.2</td><td>6.4</td><td>5.9</td><td>5.2</td><td>4.8</td><td>5.3</td><td>4.4</td><td>6.3</td>
    </tr>
    <tr>
      <td>Pixtral-12B</td>
      <td>11.3</td><td>8.9</td><td>6.1</td><td>4.0</td><td>2.6</td><td>4.2</td><td>3.3</td><td>5.8</td>
    </tr>

    <tr><td colspan="9"><strong>Open-Source ‚â§ 8B</strong></td></tr>

    <tr>
      <td>Phi-3.5-4.2B *</td>
      <td>24.4</td><td>11.2</td><td>11.4</td><td>11.1</td><td>10.3</td><td>7.4</td><td>7.1</td><td>11.5</td>
    </tr>
    <tr>
      <td>Qwen2-VL-7B *</td>
      <td>13.0</td><td>11.5</td><td>10.8</td><td>10.2</td><td>9.3</td><td>9.2</td><td>7.7</td><td>10.2</td>
    </tr>
    <tr>
      <td>Qwen2-VL-7B</td>
      <td>13.9</td><td>9.2</td><td>10.0</td><td>8.8</td><td>7.9</td><td>4.6</td><td>4.9</td><td>8.5</td>
    </tr>
    <tr>
      <td>InternVL-2.5-8B *</td>
      <td>14.8</td><td>9.6</td><td>5.7</td><td>4.8</td><td>6.3</td><td>5.7</td><td>8.2</td><td>7.9</td>
    </tr>
    <tr>
      <td>InternVL-2.5-8B *</td>
      <td>11.3</td><td>9.6</td><td>7.8</td><td>6.3</td><td>5.9</td><td>3.5</td><td>3.3</td><td>6.8</td>
    </tr>
    <tr>
      <td>Phi-3.5-4.2B</td>
      <td>5.2</td><td>7.0</td><td>6.8</td><td>6.5</td><td>6.5</td><td>7.4</td><td>4.9</td><td>6.3</td>
    </tr>
  </table>

  <p style="font-size: 14px;">
    <strong>Note:</strong> üß† Reasoning-focused; * maj@8 instead of pass@1.
  </p>
</div>
    
  </div>
  </div>
  </div>
</section>



<!-- ERROR SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <span class="mathvista" style="vertical-align: middle">Error Analysis</span>
  </h1>
  </div>
</section>



<section class="section">
  <div class="container">
    <div class="column is-full has-text-centered content">
    <div id="results-carousel" class="carousel results-carousel">
        <div class="box m-5">
        <div class="content has-text-centered">
            <img src="images/error_gemini_text.png" alt="contexts" width="80%"/>
            <p>Examples of errors made by Gemini models in text-only problems within MathGames.</p>
        </div>
        </div>
        <div class="box m-5">
        <div class="content has-text-centered">
            <img src="images/error_openai_multimodal.png" alt="contexts" width="80%"/>
            <p>Examples of errors made by OpenAI models in multimodal problems within MathGames.</p>
        </div>
        </div>
    </div>
</div>
  </div>
</section>


<!--bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@inproceedings{cocchieri-etal-2026-remedqa,
    title = "ReMedQA: Are We Done With Medical Multiple-Choice Benchmarks?",
    author = "Cocchieri, Alessio  and
      Ragazzi, Luca  and
      Tagliavini, Giuseppe  and
      Moro, Gianluca",
    booktitle = "Proceedings of the 19th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2026",
    address = "Rabat, Morocco",
    publisher = "Association for Computational Linguistics",
    abstract = "Medical multiple-choice question answering (MCQA) benchmarks report near-human accuracy, with some approaching saturation and fueling claims of clinical readiness. Yet a single accuracy score is a poor proxy for competence: models that change answers under minor perturbations cannot be considered reliable. We argue that reliability underpins accuracy‚Äîonly consistent predictions make correctness meaningful. We release ReMedQA, a benchmark suite that augments three standard medical MCQA datasets with open-answer variants and systematically perturbed items. Building on this, we introduce ReAcc and ReCon, two reliability metrics: ReAcc measures the proportion of questions answered accurately across all variations, while ReCon measures the proportion answered consistently regardless of correctness. Our evaluation shows that high MCQA accuracy masks low reliability: models remain sensitive to format and perturbation changes, and domain specialization offers no robustness gain. MCQA underestimates smaller models while inflating large ones that exploit structural cues‚Äîwith some producing correct answers without seeing the question. These findings show that, despite near-saturated accuracy, we are not yet done with medical multiple-choice benchmarks."
}</div>
</section>

<section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://disi.unibo.it/en" target="_blank" rel="external">
        <img class="center-block org-banner" src="images/unibo-logo-big.png">
    </a>
  </div>
</section>

<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>


<script>
document.addEventListener('DOMContentLoaded', () => {
  bulmaCarousel.attach('.carousel', {
    slidesToScroll: 1,
    slidesToShow: 1,
    autoplay: false,
    loop: false
  });
});
</script>
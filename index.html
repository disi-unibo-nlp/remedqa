<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ReMedQA: Are We Done with Medical Multiple-Choice Benchmarks?">
  <meta name="keywords" content="mathvista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ReMedQA: Are We Done with Medical Multiple-Choice Benchmarks?</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <span class="mathvista" style="vertical-align: middle">ReMedQA</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Are We Done with Medical Multiple-Choice Benchmarks?
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://disi-unibo-nlp.github.io/people/">Alessio Cocchieri</a>,
            </span>
            <span class="author-block">
              <a href="https://disi-unibo-nlp.github.io/people/">Luca Ragazzi</a>,
            </span>
            <span class="author-block">
              <a href="https://disi-unibo-nlp.github.io/people/">Giuseppe Tagliavini</a>,
            </span>
            <span class="author-block">
              <a href="https://disi-unibo-nlp.github.io/people/">Gianluca Moro</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Bologna, Italy</span><br>
            <span class="paper-block"><b style="color:#f41c1c">EACL 2026 Main Track</b></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/disi-unibo-nlp/remedqa"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a target="_blank" href="https://huggingface.co/datasets/disi-unibo-nlp/ReMedQA/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="https://disi-unibo-nlp.github.io/remedqa/#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üèÜ</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
              <!-- Site Link. -->
              <span class="link-block">
                <a target="_blank" href="https://disi-unibo-nlp.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <!-- üíªüîó -->
                      <p style="font-size:18px">üåê</p>
                  </span>
                  <span>Research Team</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="images/scores.png" alt="overall" width="50%" />
              <p>
                <b>LLM evaluation on ReMedQA.</b> Colored: MCQA accuracy; white: open-answer accuracy. ReAcc (green): % of questions answered accurately across all variations; ReCon (black): % of questions answered consistently (same prediction) across all variations.
              </p>
            </div>
          </div>
          
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="images/scores2.png" alt="overall2" width="40%"/>
              <p> <b>Options Only performance.</b> Each model is evaluated by providing only the MCQA answer options, without the question prompt. the red dashed line indicates the random-guess baseline.
              </p>
            </div>
          </div>

          <!-- <div class="box m-5">
            <div class="content has-text-centered">
              <img src="images/perturbations.png" alt="overall3" width="40%"/>
              <p> <b>Semantic-preserving MCQA perturbations in ReMedQA.</b> Consistent predictions across sets indicate model reliability. the correct answer is highlighted.</p>
            </p>
            </div>
          </div> -->
        </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          
        <p>
            Medical multiple-choice question answering (MCQA) benchmarks report near-human accuracy, with some approaching saturation and fueling claims of clinical readiness. Yet a single accuracy score is a poor proxy for competence: models that change answers under minor perturbations cannot be considered reliable. We argue that reliability underpins accuracy‚Äîonly consistent predictions make correctness meaningful.
            </p>
          <p> 
            We release ReMedQA, a benchmark suite that augments three standard medical MCQA datasets with open-answer variants and systematically perturbed items. Building on this, we introduce ReAcc and ReCon, two reliability metrics: ReAcc measures the proportion of questions answered accurately across all variations, while ReCon measures the proportion answered consistently regardless of correctness.
            </p>
          <p> 
            Our evaluation shows that high MCQA accuracy masks low reliability: models remain sensitive to format and perturbation changes, and domain specialization offers no robustness gain. MCQA underestimates smaller models while inflating large ones that exploit structural cues‚Äîwith some producing correct answers without seeing the question.
            </p>
           <p>
            these findings show that, despite near-saturated accuracy, we are not yet done with medical multiple-choice benchmarks. 
            </p> 
        </div>
      </div>
    </div>
</div>
</section>


<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <span class="mathvista" style="vertical-align: middle">Methodology</span>
  </h1>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered is-vcentered">

      <!-- FIGURA A SINISTRA -->
      <div class="column is-4 has-text-centered">
        <figure class="image">
          <img src="images/perturbations.png" alt="Methodology overview">
        </figure>
      </div>

      <!-- TESTO A DESTRA -->
      <div class="column is-6">
        <h2 class="title is-3">Overview</h2>

       <div class="content has-text-justified">
  <ul>
    <li>
      <strong>Fixed Position.</strong>
      We always place the gold option in position <strong>D</strong>.
      We select D because it is the least frequent gold position in the datasets
      (see Appendix&nbsp;A), minimizing positional priors while stressing
      robustness to order.
    </li>

    <li>
      <strong>No Labels.</strong>
      Standard answer labels (A/B/C/D) are removed, forcing models to output
      the content of the correct option rather than a symbol.
    </li>

    <li>
      <strong>Select Incorrect.</strong>
      Models must choose all incorrect options instead of the gold one,
      probing whether they can robustly identify distractors.
    </li>

    <li>
      <strong>Roman Numerals.</strong>
      Standard labels are replaced with
      <strong>I, II, III, IV</strong>, testing invariance to alternative
      labeling schemes.
    </li>

    <li>
      <strong>None Provided.</strong>
      The gold option is replaced with the string
      <em>‚ÄúNone of the provided options‚Äù</em>, requiring models to recognize
      when the correct answer is absent from the listed options.
    </li>
  </ul>
</div>


    </div>
  </div>
</section>



<!-- LEADERBOARD SECTION -->
<section class="hero is-light is-small" id="leaderboard">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <span class="mathvista" style="vertical-align: middle">Leaderboard on ReMedQA</span>
  </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">

   <!--      <h2 class="title is-3">Textual Problems</h2>
<div class="content">
  <b>CE, C1, C2, L1, L2, GP, HC:</b> Accuracy across age groups.  
  <br>
  <b>Avg:</b> Overall average.  
  <br> -->
  
  <table class="js-sort-table" id="results">
    <tr>
      <td colspan="1"><strong>Model</strong></td>
      <td colspan="2"><strong>Pro Med.</strong></td>
      <td colspan="2"><strong>College Med.</strong></td>
      <td colspan="2"><strong>Anatomy</strong></td>
      <td colspan="2"><strong>Clinical</strong></td>
      <td colspan="2"><strong>Biology</strong></td>
      <td colspan="2"><strong>Genetics</strong></td>
      <td colspan="2"><strong>MMLU Avg</strong></td>
      <td colspan="2"><strong>MedQA</strong></td>
      <td colspan="2"><strong>MedMCQA</strong></td>
      <td colspan="2"><strong>Avg</strong></td>
    </tr>

    <tr>
      <td colspan="1"></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
      <td><strong>ReAcc</strong></td><td><strong>ReCon</strong></td>
    </tr>

  <tbody>
    <!-- LARGE -->
     <tr><td colspan="21"><strong>Large Models</strong></td></tr>
    <tr>
      <td>GPT-5-mini üß†</td>
      <td>76.4</td><td>74.7</td>
      <td>58.3</td><td>63.6</td>
      <td>65.6</td><td>72.8</td>
      <td>61.1</td><td>65.8</td>
      <td>67.0</td><td>70.4</td>
      <td>85.4</td><td>86.6</td>
      <td>69.0</td><td>72.3</td>
      <td>65.9</td><td>64.6</td>
      <td>39.1</td><td>42.4</td>
      <td><strong>58.0</strong></td><td><strong>59.8</strong></td>
    </tr>

    <tr>
      <td>GPT-OSS-120B üß†</td>
      <td>72.0</td><td>70.9</td>
      <td>59.8</td><td>61.4</td>
      <td>54.4</td><td>57.6</td>
      <td>57.0</td><td>60.6</td>
      <td>70.6</td><td>71.6</td>
      <td>79.3</td><td>80.5</td>
      <td>65.5</td><td>67.1</td>
      <td>62.4</td><td>61.3</td>
      <td>34.4</td><td>39.3</td>
      <td><u>54.1</u></td><td><u>55.9</u></td>
    </tr>

    <tr>
      <td>Gemini-2.5-Flash üß†</td>
      <td>57.9</td><td>57.7</td>
      <td>46.2</td><td>48.9</td>
      <td>52.0</td><td>60.0</td>
      <td>46.6</td><td>48.2</td>
      <td>60.6</td><td>60.6</td>
      <td>72.0</td><td>72.0</td>
      <td>55.9</td><td>57.9</td>
      <td>47.6</td><td>47.9</td>
      <td>33.2</td><td>36.4</td>
      <td>45.6</td><td>47.4</td>
    </tr>

    <tr>
      <td>GPT-OSS-20B üß†</td>
      <td>62.6</td><td>63.1</td>
      <td>47.7</td><td>46.6</td>
      <td>52.0</td><td>56.8</td>
      <td>48.7</td><td>51.3</td>
      <td>60.6</td><td>61.5</td>
      <td>64.6</td><td>66.7</td>
      <td>56.0</td><td>57.7</td>
      <td>49.4</td><td>48.8</td>
      <td>24.9</td><td>27.2</td>
      <td>43.4</td><td>44.6</td>
    </tr>

    <tr>
      <td>Llama-3.3-70B</td>
      <td>53.5</td><td>55.3</td>
      <td>34.8</td><td>37.1</td>
      <td>48.8</td><td>51.2</td>
      <td>42.5</td><td>43.0</td>
      <td>55.0</td><td>54.1</td>
      <td>63.4</td><td>64.6</td>
      <td>49.7</td><td>50.9</td>
      <td>34.0</td><td>36.0</td>
      <td>23.6</td><td>26.6</td>
      <td>35.8</td><td>37.8</td>
    </tr>

    <!-- SMALL -->
     <tr><td colspan="21"><strong>Small Models</strong></td></tr>
    <tr>
      <td>Llama-3-8B</td>
      <td>9.8</td><td>17.7</td>
      <td>9.8</td><td>14.4</td>
      <td>12.8</td><td>27.2</td>
      <td>11.9</td><td>19.2</td>
      <td>15.6</td><td>25.7</td>
      <td>23.2</td><td>35.4</td>
      <td>13.9</td><td>23.3</td>
      <td>6.1</td><td>11.5</td>
      <td>6.8</td><td>14.8</td>
      <td>8.9</td><td>16.5</td>
    </tr>

    <tr>
      <td>Llama3-Med42-8B ü©∫</td>
      <td>15.7</td><td>18.5</td>
      <td>18.2</td><td>23.8</td>
      <td>20.8</td><td>28.0</td>
      <td>17.6</td><td>20.2</td>
      <td>15.6</td><td>22.0</td>
      <td>24.4</td><td>34.1</td>
      <td>18.7</td><td>24.4</td>
      <td>9.1</td><td>13.0</td>
      <td>10.8</td><td>16.7</td>
      <td>12.9</td><td>18.0</td>
    </tr>

    <tr>
      <td>Phi-3.5-mini</td>
      <td>23.6</td><td>24.4</td>
      <td>9.8</td><td>13.6</td>
      <td>26.4</td><td>30.4</td>
      <td>18.1</td><td>19.2</td>
      <td>29.4</td><td>30.3</td>
      <td>19.5</td><td>19.5</td>
      <td>21.1</td><td>22.9</td>
      <td>12.3</td><td>14.5</td>
      <td>11.0</td><td>12.1</td>
      <td>14.8</td><td>16.5</td>
    </tr>

    <tr>
      <td>MediPhi-3.8B ü©∫</td>
      <td>11.8</td><td>14.6</td>
      <td>14.4</td><td>18.2</td>
      <td>20.0</td><td>23.2</td>
      <td>18.7</td><td>18.7</td>
      <td>21.1</td><td>21.1</td>
      <td>24.4</td><td>23.2</td>
      <td>18.4</td><td>19.8</td>
      <td>8.6</td><td>12.6</td>
      <td>7.9</td><td>9.6</td>
      <td>11.6</td><td>14.0</td>
    </tr>

    <tr>
      <td>Gemma-3-4B</td>
      <td>5.9</td><td>8.3</td>
      <td>5.3</td><td>9.1</td>
      <td>16.0</td><td>19.2</td>
      <td>6.7</td><td>8.3</td>
      <td>12.8</td><td>16.5</td>
      <td>23.2</td><td>23.8</td>
      <td>11.7</td><td>14.2</td>
      <td>3.3</td><td>6.0</td>
      <td>3.8</td><td>6.4</td>
      <td>6.3</td><td>8.9</td>
    </tr>

    <tr>
      <td>MedGemma-4B ü©∫</td>
      <td>2.8</td><td>4.3</td>
      <td>2.3</td><td>4.5</td>
      <td>4.8</td><td>10.4</td>
      <td>3.6</td><td>5.2</td>
      <td>6.4</td><td>8.3</td>
      <td>8.5</td><td>7.3</td>
      <td>4.7</td><td>6.7</td>
      <td>2.5</td><td>5.3</td>
      <td>2.7</td><td>5.2</td>
      <td>3.3</td><td>5.7</td>
    </tr>
  </tbody>
</table>


  <p style="font-size: 14px;">
    <strong>Note:</strong> üß† Reasoning-focused LLMs; ü©∫ Medical-specialized LLMs.
  </p>
</div>

      </div>
    </div>

  </div>
</section>



<!--bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@inproceedings{cocchieri-etal-2026-remedqa,
    title = "ReMedQA: Are We Done with Medical Multiple-Choice Benchmarks?",
    author = "Cocchieri, Alessio  and
      Ragazzi, Luca  and
      Tagliavini, Giuseppe  and
      Moro, Gianluca",
    booktitle = "Proceedings of the 19th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2026",
    address = "Rabat, Morocco",
    publisher = "Association for Computational Linguistics",
    abstract = "Medical multiple-choice question answering (MCQA) benchmarks report near-human accuracy, with some approaching saturation and fueling claims of clinical readiness. Yet a single accuracy score is a poor proxy for competence: models that change answers under minor perturbations cannot be considered reliable. We argue that reliability underpins accuracy‚Äîonly consistent predictions make correctness meaningful. We release ReMedQA, a benchmark suite that augments three standard medical MCQA datasets with open-answer variants and systematically perturbed items. Building on this, we introduce ReAcc and ReCon, two reliability metrics: ReAcc measures the proportion of questions answered accurately across all variations, while ReCon measures the proportion answered consistently regardless of correctness. Our evaluation shows that high MCQA accuracy masks low reliability: models remain sensitive to format and perturbation changes, and domain specialization offers no robustness gain. MCQA underestimates smaller models while inflating large ones that exploit structural cues‚Äîwith some producing correct answers without seeing the question. these findings show that, despite near-saturated accuracy, we are not yet done with medical multiple-choice benchmarks."
}</div>
</section>

<section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://disi.unibo.it/en" target="_blank" rel="external">
        <img class="center-block org-banner" src="images/unibo-logo-big.png">
    </a>
  </div>
</section>

<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            this website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>


<script>
document.addEventListener('DOMContentLoaded', () => {
  bulmaCarousel.attach('.carousel', {
    slidesToScroll: 1,
    slidesToShow: 1,
    autoplay: false,
    loop: false
  });
});
</script>